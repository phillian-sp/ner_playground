{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "raw_dataset = json.load(open(\"../../data/final_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4126/4126 [00:00<00:00, 757858.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Then Sarah had an idea, another dream.',\n",
       " 'labels': [{'start': 5,\n",
       "   'end': 10,\n",
       "   'label': 'Biological Kind',\n",
       "   'word': 'Sarah'},\n",
       "  {'start': 32, 'end': 37, 'label': 'Mental State', 'word': 'dream'},\n",
       "  {'start': 18, 'end': 22, 'label': 'Mental State', 'word': 'idea'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {}\n",
    "for data in raw_dataset:\n",
    "    sentence_id, sentence = data[\"sentence_id\"], data[\"sentence\"]\n",
    "    if sentence_id not in dataset:\n",
    "        dataset[sentence_id] = {\"sentence\": sentence, \"sentence_id\": sentence_id, \"labels\": []}\n",
    "        \n",
    "    dataset[sentence_id][\"labels\"].append({\n",
    "        \"word\": data[\"word\"],\n",
    "        \"label\": data[\"label\"],\n",
    "        \"start\": data[\"start\"], \n",
    "        \"end\": data[\"end\"],\n",
    "    })\n",
    "dataset[416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', '##ic', '##y', '##cle']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"unicycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Artifacts\",\n",
    "    2: \"I-Artifacts\",\n",
    "    3: \"B-Behavioral\",\n",
    "    4: \"I-Behavioral\",\n",
    "    5: \"B-Biological Kind\",\n",
    "    6: \"I-Biological Kind\",\n",
    "    7: \"B-Mental State\",\n",
    "    8: \"I-Mental State\",\n",
    "    9: \"B-Non-Living Kind\",\n",
    "    10: \"I-Non-Living Kind\",\n",
    "    11: \"B-Normative Feature\",\n",
    "    12: \"I-Normative Feature\",\n",
    "    13: \"B-Perceptual\",\n",
    "    14: \"I-Perceptual\",\n",
    "    15: \"B-Social Kind/Role\",\n",
    "    16: \"I-Social Kind/Role\",\n",
    "    17: \"B-Anthropomorphized\",\n",
    "    18: \"I-Anthropomorphized\",\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'un', '##ic', '##y', '##cle', '.', '[SEP]']\n",
      "[-100, 'B-Artifacts', -100, -100, -100, 'O', -100]\n",
      "[101, 4895, 2594, 2100, 14321, 1012, 102]\n",
      "[-100, 1, -100, -100, -100, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(data):\n",
    "    encoded = tokenizer.encode_plus(data['sentence'], return_offsets_mapping=True)\n",
    "    ids = encoded[\"input_ids\"]\n",
    "    offsets = encoded[\"offset_mapping\"]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    mask = encoded[\"attention_mask\"]\n",
    "    result = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        result.append({\n",
    "            \"token\": token,\n",
    "            \"start_index\": offsets[i][0],\n",
    "            \"end_index\": offsets[i][1],\n",
    "            \"id\": ids[i],\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def most_frequent(list_of_labels):\n",
    "   return max(set(list_of_labels), key=list_of_labels.count)\n",
    "    \n",
    "    \n",
    "def get_clean_label(label):\n",
    "    if label == \"O\":\n",
    "        return label\n",
    "    else:\n",
    "        return label.split(\" @@@ \")[0]\n",
    "\n",
    "\n",
    "def generate_labeled_tokens(data):\n",
    "    text = data[\"sentence\"]\n",
    "    labels = data[\"labels\"]\n",
    "    tokens = tokenize(data)\n",
    "\n",
    "    char_label = [\"O\"] * len(text)\n",
    "\n",
    "    for i, span in enumerate(labels):\n",
    "\n",
    "        label = span[\"label\"]\n",
    "        start = span[\"start\"]\n",
    "        end = span[\"end\"]\n",
    "\n",
    "        char_label[start:end] = [f\"{label} @@@ #{i}\"] * (end - start)\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[\"start_index\"] != token[\"end_index\"]:\n",
    "            token[\"raw_label\"] = most_frequent(\n",
    "                char_label[token[\"start_index\"] : token[\"end_index\"]]\n",
    "            )\n",
    "        else:\n",
    "            token[\"raw_label\"] = \"O\"\n",
    "        token[\"clean_label\"] = get_clean_label(token[\"raw_label\"])\n",
    "        \n",
    "    # BIO labels\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[\"raw_label\"] != \"O\":\n",
    "            if i == 0:\n",
    "                token[\"bio_label\"] = \"B-\" + token[\"clean_label\"]\n",
    "\n",
    "            else:\n",
    "                if tokens[i - 1][\"raw_label\"] == tokens[i][\"raw_label\"]:\n",
    "                    token[\"bio_label\"] = \"I-\" + token[\"clean_label\"]\n",
    "                else:\n",
    "                    token[\"bio_label\"] = \"B-\" + token[\"clean_label\"]\n",
    "        else:\n",
    "            token[\"bio_label\"] = \"O\"\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_list(sentence_id, tokens):\n",
    "    ner_tags = []\n",
    "    tokens_list = []\n",
    "    token_ids = []\n",
    "    label_ids = []\n",
    "    for token in tokens:\n",
    "        tokens_list.append(token[\"token\"])\n",
    "        token_ids.append(token[\"id\"])\n",
    "        if (token[\"token\"].startswith(\"##\")) or (token[\"token\"] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "            label_ids.append(-100)\n",
    "            ner_tags.append(-100)\n",
    "        else:\n",
    "            label_ids.append(label2id[token[\"bio_label\"]])\n",
    "            ner_tags.append(token[\"bio_label\"])\n",
    "\n",
    "    return {\n",
    "        \"sentence_id\": sentence_id,\n",
    "        \"tokens\": tokens_list,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"label_ids\": label_ids,\n",
    "    }\n",
    "\n",
    "result = tokens_to_list(459, generate_labeled_tokens(dataset[459]))\n",
    "print(result['tokens'])\n",
    "print(result['ner_tags'])\n",
    "print(result['token_ids'])\n",
    "print(result['label_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence_id in dataset:\n",
    "    result = tokens_to_list(sentence_id, generate_labeled_tokens(dataset[sentence_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
